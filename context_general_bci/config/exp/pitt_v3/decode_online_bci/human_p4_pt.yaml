# @package _global_
defaults:
  - _default
train:
  autoscale_batch_size: True # For some unknown reason, this is critical to perf. Model is brittle to training noise (not enough data)
  # autoscale_batch_size: false
  patience: 200
  max_batch_size: 32
dataset:
  augmentations:
  - rand_crop_time # preset to crop to 1s
  datasets:
  - observation_P4.*
  exclude_datasets: []
model:
  transformer:
    n_layers: 6
  task:
    mask_ratio: 0.1
  # lr_ramp_steps: 100
inherit_exp: ""
inherit_tag: ""