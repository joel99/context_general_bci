# @package _global_

defaults:
  - /model: flat_enc_dec
  - /model/task:
    - joint_bhvr_decode_flat
  - /dataset: flat
model:
  lr_ramp_steps: 50
  lr_decay_steps: 1500
  lr_interval: epoch
  causal: true
  neurons_per_token: 32
  session_embed_strategy: EmbedStrat.token
  transformer:
    max_trial_length: 200 # 2 seconds
    n_layers: 10 # For pitt_v3/human_10l
    # n_layers: 6
    max_spatial_tokens: 9 # Support loading of pt model
  task:
    mask_ratio: 0.25 # for efficiency
    task_weights: [1.0, 1.0]
    decode_time_pool: ''
    decode_normalizer: './data/zscore_h1_7d.pt'
  decoder_context_integration: 'cross_attn'
  dropout: 0.1

dataset:
  bin_size_ms: 20
  max_length_ms: 4000
  max_tokens: 8192 # expecting 6 tokens/bin x 500 bins = 3000 tokens
  explicit_alias_to_session: true

  neurons_per_token: 32
  max_channels: 288 # high, just to match PT
  max_arrays: 1

  data_keys:
  - DataKey.spikes
  - DataKey.bhvr_vel
  - DataKey.bhvr_mask
  behavior_dim: 7
  datasets:
  - falcon_FALCONH1.*_calib
train:
  max_batch_size: 32
  batch_size: 32
  patience: 100 # match ndt3 v5
  # patience: 250 # ndt3 v4
  early_stop_metric: val_kinematic_r2
effective_bsz: 32

# sweep_cfg: 'simple_scratch'
sweep_cfg: 'simple_tune'
sweep_mode: 'grid'

# init_ckpt: './data/pretrained/f32_uych1wae_val-epoch=630-val_loss=0.5030.ckpt' # Can't use this one in general because it bleeds Indy data. Also, fails to load due to codespace imports.
init_ckpt: './data/pretrained/pretrained_unsup_j7mq2snc_val-epoch=960-val_loss=0.3158.ckpt'