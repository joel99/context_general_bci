# @package _global_

tag: dyer
# Testing for utility of spatial tokenization - this one should be relatively efficient, in that we can serve Pitt + Chewie together
# ! Note when using Pitt data, it's important to go up to 200 spatial tokens (do the math to compute token budget)
defaults:
  - /model: pretrain_2x
  - /train: pretrain
dataset:
  serve_tokenized: true
  # serve_tokenized: false
  bin_size_ms: 20
  max_channels: 192
  max_length_ms: 4000
  max_arrays: 2
  datasets:
  # - CRS02bHome.data.*
  - dyer_co.*
  eval_datasets:
  - dyer_co_chewie_2
  data_keys:
  - DataKey.spikes
  pitt_co:
    arrays:
    - CRS02b-lateral_m1
    - CRS02b-medial_m1
  meta_keys:
  - MetaKey.unique
  - MetaKey.session
  - MetaKey.array
  - MetaKey.subject
  neurons_per_token: 4
  max_tokens: 4096
model:
  neurons_per_token: 4
  transform_space: true
  transformer:
    factorized_space_time: true

  subject_embed_strategy: EmbedStrat.token
  array_embed_strategy: EmbedStrat.none
  spike_embed_style: EmbedStrat.token