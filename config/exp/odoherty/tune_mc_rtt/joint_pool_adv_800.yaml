# @package _global_
defaults:
  - /model:
    - flat_enc_dec
    # - accel_tune
  - /model/task:
    - joint_bhvr_decode_flat
  - /train: finetune
  - /dataset: flat
model:
  session_embed_token_count: 8
  subject_embed_strategy: EmbedStrat.token
  neurons_per_token: 16
  causal: true

  task:
    behavior_lag: 120
    task_weights: [1.0, 20.0] # decode goes to ~0.06-0.12 with 20x weight, vs infill at ~0.25-0.3.
    decode_time_pool: "mean"

    blacklist_session_supervision: ['mc_rtt']
    adversarial_classify_lambda: 0.01


dataset:
  max_tokens: 8192
  max_length_ms: 2000 # fit
  max_arrays: 1

  neurons_per_token: 16
  max_channels: 288

  data_keys:
  - DataKey.spikes
  - DataKey.bhvr_vel

  datasets:
  # - odoherty_rtt-Indy-20160624_03
  - odoherty_rtt-Indy.*
  - mc_rtt
  eval_datasets:
  # - odoherty_rtt-Indy-20161005_06
  - mc_rtt
  # - odoherty_rtt-Indy-20160624_03

train:
  autoscale_batch_size: false
  batch_size: 32 # mind 24G nodes
init_from_id: indy_causal_joint_pool-ghu9iaw9
init_tag: val_loss