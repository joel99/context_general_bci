# @package _global_
tag: maze_med_parity

defaults:
  - /model: pretrain_small
  - /dataset: maze_nlb
  - /train: nlb
model:
  readin_strategy: EmbedStrat.token
  dropout: 0.5
  hidden_size: 128
  session_embed_size: 128
  transformer:
    n_layers: 6
    n_state: 128
    n_heads: 2
    dropout: 0.5
  task:
    mask_ratio: 0.25
    mask_random_ratio: 0.0
dataset:
  datasets:
  - mc_maze_medium
train:
  epochs: 50000
  batch_size: 128
  patience: 500 # batches are smaller