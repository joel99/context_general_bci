# @package _global_
# We would like to smoothly ablate spatial chunk size (neurons per token); we expect this to hurt performance.
# But less for scaled data.
# Running with 1 4 16; we don't want the batch to be too small

defaults:
  - /model: pretrain_2x
  - /dataset: flat # just convenient, we override below
model:
  subject_embed_strategy: EmbedStrat.token
  array_embed_strategy: EmbedStrat.none
  task:
    mask_ratio: 0.5 # 0.25 is quite costly, but we want to see long tail of improvements, so go over 0.8
dataset:
  max_tokens: 4096 # fit
  max_length_ms: 2000 # fit
  max_arrays: 1
  scale_ratio: 1.0

  serve_tokenized: False
  serve_tokenized_flat: False


  max_channels: 288
  bin_size_ms: 20
  datasets:
  - odoherty_rtt-Loco.*
  - odoherty_rtt-Indy-20161005_06
  eval_datasets:
  - odoherty_rtt-Indy-20161005_06
