# @package _global_
# We would like to smoothly ablate spatial chunk size (neurons per token); we expect this to hurt performance.
# But less for scaled data.
# Running with 1 4 16; we don't want the batch to be too small

defaults:
  - /model: flat_enc_dec
  - /dataset: flat
model:
  subject_embed_strategy: EmbedStrat.token
  array_embed_strategy: EmbedStrat.none
  task:
    mask_ratio: 0.5 # 0.25 is quite costly, but we want to see long tail of improvements, so go over 0.8
  neurons_per_token: 128
dataset:
  max_tokens: 4096 # fit
  max_length_ms: 2000 # fit
  max_arrays: 1
  scale_ratio: 1.0

  neurons_per_token: 128

  max_channels: 384
  bin_size_ms: 20
  datasets:
  - odoherty_rtt-Loco.*
  - odoherty_rtt-Indy-20161005_06
  eval_datasets:
  - odoherty_rtt-Indy-20161005_06
train:
  patience: 300
load_from_id: loco_t128-zs40song
