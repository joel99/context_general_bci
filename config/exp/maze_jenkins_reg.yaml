# @package _global_
tag: maze_jenkins

defaults:
  - /model: pretrain
  - /dataset: maze_nlb
  - /train: pretrain

model:
  array_embed_strategy: EmbedStrat.token_add
  dropout: 0.4
  transformer:
    dropout: 0.4
dataset:
  datasets:
  - churchland_maze_jenkins.*
  - mc_maze$
  data_keys:
  - DataKey.spikes
  max_arrays: 2
train:
  # moving to 2 GPU
  # accumulate_batches: 2
  batch_size: 128