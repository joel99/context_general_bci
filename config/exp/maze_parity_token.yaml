# @package _global_

# Design to match `mc_maze` from NDT 1 as closely as possible
tag: maze_parity_token

defaults:
  - /model: pretrain_small
  - /dataset: maze_nlb
  - /train: nlb
model:
  readin_strategy: EmbedStrat.project
  session_embed_strategy: EmbedStrat.none
  # readin_strategy: EmbedStrat.token
  dropout: 0.5
  hidden_size: 128
  session_embed_size: 128
  transformer:
    n_layers: 4
    n_state: 128
    n_heads: 2
    dropout: 0.5
    activation: "relu"
  task:
    mask_ratio: 0.25
    mask_token_ratio: 0.8
    mask_random_ratio: 0.1
  weight_decay: 5e-5
  lr_init: 1e-3
  lr_ramp_steps: 5000
dataset:
  datasets:
  - mc_maze$
  max_channels: 0
train:
  epochs: 50000
  batch_size: 64
  patience: 3000 # batches are smaller

# Unimplemented, missing elements
# - context forward and backward
# - fixup init, pre-norm** (plausibly most important candidate right now, but still... really? we're in a _completely_ wrong regime of overfitting right now)
# 1. cosine lr schedule (precise warmup steps)
# 2. mask span
# 3. not half precision
# 4. learnable position
# 6. trial length cropped to 140
# 7. grad clip, scale factor after readin