# @package _global_
tag: maze_all_ctx

defaults:
  - /model: pretrain
  - /dataset: maze
  - /train: pretrain

model:
  subject_embed_strategy: EmbedStrat.token
  array_embed_strategy: EmbedStrat.token_add

  readin_strategy: EmbedStrat.contextual_mlp
  readout_strategy: EmbedStrat.contextual_mlp
  readin_compress: False
  readin_dim: 128
  readout_dim: 128
dataset:
  eval_datasets:
  - mc_maze_medium
  # task:
  #   unique_no_head: True
# train:
  # patience: 300
# load_from_id: "maze_all-22wt8z8p"