# @package _global_
tag: maze_all_stitch_256

defaults:
  - /model: pretrain
  - /dataset: maze
  - /train: pretrain

model:
  subject_embed_strategy: EmbedStrat.token
  array_embed_strategy: EmbedStrat.token_add

  readin_strategy: EmbedStrat.unique_project
  readout_strategy: EmbedStrat.unique_project
  readin_compress: False
  hidden_size: 256
# train:
  # patience: 300
# load_from_id: "maze_all-22wt8z8p"
dataset:
  eval_datasets:
  - mc_maze_medium