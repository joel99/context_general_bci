# @package _global_
tag: rtt_nlb

defaults:
  - /model: pretrain
  - /dataset: rtt_nlb
  - /train: pretrain
model:
  subject_embed_strategy: EmbedStrat.token
  array_embed_strategy: EmbedStrat.token_add

  lr_ramp_steps: 500 # because tiny batches
  lr_decay_steps: 5000
dataset:
  datasets:
  - mc_rtt
  data_keys:
  - DataKey.spikes
  meta_keys:
  - MetaKey.unique
  - MetaKey.session
  - MetaKey.array
  - MetaKey.subject
train:
  epochs: 50000
  batch_size: 128 # ~10G
  patience: 500