# @package _global_

defaults:
  - /model: pretrain_small
  - /train: nlb
  - /dataset: maze_nlb
dataset:
  datasets:
  - mc_maze_med
  eval_datasets:
  - mc_maze_medium
  eval_ratio: 0.2
  nlb_maze:
    heldout_neurons: 38
  data_keys:
  - DataKey.spikes
  - DataKey.heldout_spikes
  bin_size_ms: 20
  max_channels: 128
model:
  session_embed_strategy: EmbedStrat.none
  dropout: 0.8
  transformer:
    dropout: 0.8
    n_heads: 2
  task:
    mask_token_ratio: 1.0 # for parity with shuffle
    task_weights: [1., 1.]
    tasks:
    - ModelTask.infill
    - ModelTask.heldout_decoding
    metrics:
    - Metric.bps
    - Metric.co_bps
    - Metric.block_co_bps
train:
  patience: 5000
seed: 2
notes: "Reference 0. The goal"
# This switches masking strategy.