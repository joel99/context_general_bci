# @package _global_
tag: rtt_nlb

model:
  task:
    task: ModelTask.infill
  readin_strategy: EmbedStrat.token
  lr_schedule: 'cosine_warmup'
  lr_init: 5e-4
  lr_ramp_steps: 500 # because tiny batches
  lr_decay_steps: 5000
dataset:
  bin_size_ms: 5
  datasets:
  - mc_rtt
  max_channels: 98
  max_arrays: 1
train:
  epochs: 50000
  batch_size: 128 # batch size reduced here since there's only ~1K trials
  patience: 500 # smaller batches
  # batch_size: 512
