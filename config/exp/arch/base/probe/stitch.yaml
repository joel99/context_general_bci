# @package _global_

defaults:
  - /model: pretrain_2x # equivalent to flat_enc_dec without flattening
  - /model/task:
    - bhvr_decode
  - /dataset: flat # We override rather than use different preset for easy comparison
model:
  causal: true
  subject_embed_strategy: EmbedStrat.token
  task:
    mask_ratio: 0.5 # for efficiency

    behavior_lag: 120

  readin_strategy: EmbedStrat.unique_project
  readout_strategy: EmbedStrat.unique_project
  readin_compress: False
  readin_dim: 256
  readout_dim: 256
  # Different than `saturation` pilots, we also provide context tokens here.

  lr_init: 5e-5
  lr_ramp_steps: 1000
  lr_decay_steps: 10000
  accelerate_new_params: 10.0
  tune_decay: 0.75 # per Kaiming MAE
dataset:
  serve_tokenized: False
  serve_tokenized_flat: False

  max_arrays: 1
  max_channels: 288

  scale_ratio: 1.0
  scale_limit_per_eval_session: 300 # no limit

  data_keys:
  - DataKey.spikes
  - DataKey.bhvr_vel

  datasets:
  - odoherty_rtt-Indy-20160627_01
  eval_datasets:
  - odoherty_rtt-Indy-20160627_01
train:
  patience: 250 # Extra generous patience.
  autoscale_batch_size: False
  batch_size: 16

init_from_id: stitch-3u4jt5uq
init_tag: val_loss

notes: "Sorted, Stitched NDT1 (time only). ~2.5M params for encoder, 5M params for stitching IO. Autobsz ~2048."