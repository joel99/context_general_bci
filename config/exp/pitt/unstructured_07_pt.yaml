# @package _global_

defaults:
  - /model: flat_enc_dec # equivalent to flat_enc_dec without flattening
  - /dataset: flat # We override rather than use different preset for easy comparison
model:
  causal: true
  task:
    mask_ratio: 0.5 # for efficiency
  session_embed_token_count: 8
  subject_embed_strategy: EmbedStrat.token
  task_embed_strategy: EmbedStrat.token
  neurons_per_token: 16
dataset:
  neurons_per_token: 16

  max_arrays: 2
  max_channels: 192

  scale_ratio: 1.0
  # scale_limit_per_eval_session: 300 # no limit

  datasets:
  - unstructured.*
  - obs.*
  - ortho.*
  - fbc.*
  - CRS02bHome.data.00437
  eval_datasets:
  - CRS02bHome.data.00437
  # eval_datasets:
  # - odoherty_rtt-Indy-20160627_01
train:
  patience: 250 # Extra generous patience.
  # autoscale_batch_size: False
  # batch_size: 512
# notes: "Sorted, NDT1 (time only)"