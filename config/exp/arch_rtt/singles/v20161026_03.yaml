# @package _global_

defaults:
  - /model: pretrain_small
  - /dataset: maze_nlb
  - /train: nlb
dataset:
  datasets:
  - odoherty_rtt-Indy-20161026_03
  eval_datasets:
  - odoherty_rtt-Indy-20161026_03
  data_keys:
  - DataKey.spikes
  # - DataKey.heldout_spikes
  bin_size_ms: 20
  odoherty_rtt:
    arrays: ['Indy-M1_all', 'Loco-M1_all']
    include_sorted: True
  # nlb_maze:
    # heldout_neurons: 0
  # nlb_rtt:
    # heldout_neurons: 32
  scale_limit_per_eval_session: 300 # no limit
  max_channels: 288
  eval_ratio: 0.1
model:
  dropout: 0.5
  transformer:
    dropout: 0.5
    n_heads: 2
  task:
    task_weights: [1., 1.]
    tasks:
    - ModelTask.infill
    # - ModelTask.heldout_decoding
    metrics:
    - Metric.bps
    # - Metric.co_bps
    # - Metric.block_co_bps
train:
  autoscale_batch_size: false
  batch_size: 64
  patience: 5000