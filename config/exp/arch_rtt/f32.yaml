# @package _global_

defaults:
  - /model: flat_enc_dec
  - /dataset: flat
model:
  causal: false
  subject_embed_strategy: EmbedStrat.token
  task:
    mask_ratio: 0.25 # for efficiency
  neurons_per_token: 32
dataset:
  neurons_per_token: 32

  max_arrays: 1
  max_channels: 288

  scale_ratio: 1.0
  scale_limit_per_eval_session: 300 # no limit

  datasets:
  - odoherty_rtt-Indy.*
  eval_datasets:
  - odoherty_rtt-Indy-20160627_01

train:
  accumulate_batches: 2 # auto = 128 (12G GPUs), effective = 256
  autoscale_batch_size: false
  batch_size: 128

notes: "Sorted, Patch size 32"