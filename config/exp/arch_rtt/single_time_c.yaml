# @package _global_

defaults:
  - /model: pretrain_2x # equivalent to flat_enc_dec without flattening
  - /dataset: flat # We override rather than use different preset for easy comparison
model:
  lr_ramp_steps: 3000
  lr_decay_steps: 100000
  # Model doesn't train without slower ramp.
  causal: false
  subject_embed_strategy: EmbedStrat.token
  task:
    mask_ratio: 0.25 # for efficiency
  hidden_size: 128
  dropout: 0.6
dataset:
  serve_tokenized: False
  serve_tokenized_flat: False

  max_arrays: 1
  max_channels: 245 # # for this single session

  scale_ratio: 1.0
  scale_limit_per_eval_session: 300 # no limit

  datasets:
  - odoherty_rtt-Indy-20160627_01
  eval_datasets:
  - odoherty_rtt-Indy-20160627_01
train:
  patience: 600 # Extraaaa generous patience. Since this guy is not learning for some reason.
  autoscale_batch_size: False
  batch_size: 64
notes: "Trying to pin down source of smoothness in `single_time_nlb_r300."
