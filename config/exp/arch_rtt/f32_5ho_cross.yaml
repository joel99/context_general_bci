# @package _global_

defaults:
  - /model: flat_enc_dec
  - /dataset: flat
model:
  causal: false
  subject_embed_strategy: EmbedStrat.token
  transformer:
    context_integration: cross_attn
    debug_force_nonlearned_position: True
  task:
    mask_ratio: 0.25 # for efficiency
  neurons_per_token: 32
dataset:
  neurons_per_token: 32

  max_arrays: 1
  max_channels: 288

  scale_ratio: 1.0
  scale_limit_per_eval_session: 300

  datasets:
  - odoherty_rtt-Indy.*
  eval_datasets:
  # just taking the first row of each colum in my terminal :)
  - odoherty_rtt-Indy-20160407_02 # First indy session
  - odoherty_rtt-Indy-20160627_01 # Original
  - odoherty_rtt-Indy-20161005_06
  - odoherty_rtt-Indy-20161026_03
  - odoherty_rtt-Indy-20170131_02 # Last indy sesison
  eval_ratio: 0.1 # used smaller since not all sessions have (1/0.5) * 300 trials

train:
  patience: 300
  accumulate_batches: 2 # auto = 128 (12G GPUs), effective = 2048
  # autoscale_batch_size: false
  # batch_size: 128
load_from_id: f32_5ho_cross-113u9ky8
notes: "Sorted, Patch size 32"
